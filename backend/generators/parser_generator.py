from pathlib import Path
from typing import List
from models.language_spec import LanguageSpecification


class ParserGenerator:
    """Generates parser code for the language"""

    def __init__(self, spec: LanguageSpecification):
        self.spec = spec

    def generate(self, output_dir: Path) -> List[str]:
        """Generate parser files"""
        generated_files = []

        # Generate grammar file (EBNF format)
        grammar_file = output_dir / f"{self.spec.name.lower()}.ebnf"
        with open(grammar_file, 'w') as f:
            f.write(self._generate_grammar())
        generated_files.append(str(grammar_file))

        # Generate lexer
        lexer_file = output_dir / "lexer.py"
        with open(lexer_file, 'w') as f:
            f.write(self._generate_lexer())
        generated_files.append(str(lexer_file))

        # Generate parser
        parser_file = output_dir / "parser.py"
        with open(parser_file, 'w') as f:
            f.write(self._generate_parser())
        generated_files.append(str(parser_file))

        # Generate AST node definitions
        ast_file = output_dir / "ast_nodes.py"
        with open(ast_file, 'w') as f:
            f.write(self._generate_ast_nodes())
        generated_files.append(str(ast_file))

        return generated_files

    def _generate_grammar(self) -> str:
        """Generate EBNF grammar specification"""
        grammar = f"(* Grammar for {self.spec.name} *)\n\n"

        # Start symbol
        grammar += "program = statement* ;\n\n"

        # Add custom grammar rules
        for rule in self.spec.grammar_rules:
            grammar += f"{rule.name} = {rule.pattern} ;\n"

        grammar += "\n"

        # Add syntax rules as grammar productions
        for syntax_rule in self.spec.syntax_rules:
            grammar += f"{syntax_rule.rule_type} = {syntax_rule.pattern} ;\n"

        grammar += "\n"

        # Add keyword definitions
        if self.spec.keywords:
            grammar += "(* Keywords *)\n"
            for keyword in self.spec.keywords:
                grammar += f"(* {keyword.word}: {keyword.description} *)\n"

        return grammar

    def _generate_lexer(self) -> str:
        """Generate lexer/tokenizer"""
        code = '''"""
Lexer for {name}
Auto-generated by Illiterate Wizard
"""

import re
from enum import Enum, auto
from dataclasses import dataclass
from typing import List, Optional


class TokenType(Enum):
    # Keywords
{keyword_tokens}

    # Operators
{operator_tokens}

    # Literals
    INTEGER = auto()
    FLOAT = auto()
    STRING = auto()
    BOOLEAN = auto()

    # Identifiers and symbols
    IDENTIFIER = auto()
    LPAREN = auto()
    RPAREN = auto()
    LBRACE = auto()
    RBRACE = auto()
    LBRACKET = auto()
    RBRACKET = auto()
    SEMICOLON = auto()
    COMMA = auto()
    DOT = auto()

    # Special
    NEWLINE = auto()
    EOF = auto()
    WHITESPACE = auto()
    COMMENT = auto()


@dataclass
class Token:
    type: TokenType
    value: str
    line: int
    column: int


class Lexer:
    def __init__(self, source: str):
        self.source = source
        self.pos = 0
        self.line = 1
        self.column = 1
        self.tokens: List[Token] = []

        # Build keyword map
        self.keywords = {{
{keyword_map}
        }}

        # Build operator map
        self.operators = {{
{operator_map}
        }}

    def tokenize(self) -> List[Token]:
        """Tokenize the source code"""
        while self.pos < len(self.source):
            self._skip_whitespace()

            if self.pos >= len(self.source):
                break

            # Try to match each token type
            if self._match_comment():
                continue
            elif self._match_number():
                continue
            elif self._match_string():
                continue
            elif self._match_operator():
                continue
            elif self._match_identifier_or_keyword():
                continue
            elif self._match_symbol():
                continue
            else:
                raise SyntaxError(f"Unexpected character '{{self.source[self.pos]}}' at line {{self.line}}, column {{self.column}}")

        self.tokens.append(Token(TokenType.EOF, '', self.line, self.column))
        return self.tokens

    def _skip_whitespace(self):
        """Skip whitespace characters"""
        while self.pos < len(self.source) and self.source[self.pos] in ' \\t\\r\\n':
            if self.source[self.pos] == '\\n':
                self.line += 1
                self.column = 1
            else:
                self.column += 1
            self.pos += 1

    def _match_comment(self) -> bool:
        """Match comment syntax"""
        single_line = {single_line_comment}
        if self.source[self.pos:self.pos+len(single_line)] == single_line:
            while self.pos < len(self.source) and self.source[self.pos] != '\\n':
                self.pos += 1
            return True
        return False

    def _match_number(self) -> bool:
        """Match integer or float"""
        start = self.pos
        start_col = self.column

        if not self.source[self.pos].isdigit():
            return False

        while self.pos < len(self.source) and self.source[self.pos].isdigit():
            self.pos += 1
            self.column += 1

        # Check for float
        if self.pos < len(self.source) and self.source[self.pos] == '.':
            self.pos += 1
            self.column += 1

            if not (self.pos < len(self.source) and self.source[self.pos].isdigit()):
                raise SyntaxError(f"Invalid number at line {{self.line}}, column {{start_col}}")

            while self.pos < len(self.source) and self.source[self.pos].isdigit():
                self.pos += 1
                self.column += 1

            value = self.source[start:self.pos]
            self.tokens.append(Token(TokenType.FLOAT, value, self.line, start_col))
        else:
            value = self.source[start:self.pos]
            self.tokens.append(Token(TokenType.INTEGER, value, self.line, start_col))

        return True

    def _match_string(self) -> bool:
        """Match string literals"""
        if self.source[self.pos] not in ['"', "'"]:
            return False

        quote = self.source[self.pos]
        start = self.pos
        start_col = self.column
        self.pos += 1
        self.column += 1

        value = ""
        while self.pos < len(self.source) and self.source[self.pos] != quote:
            if self.source[self.pos] == '\\\\':
                self.pos += 1
                self.column += 1
                if self.pos < len(self.source):
                    value += self.source[self.pos]
                    self.pos += 1
                    self.column += 1
            else:
                value += self.source[self.pos]
                self.pos += 1
                self.column += 1

        if self.pos >= len(self.source):
            raise SyntaxError(f"Unterminated string at line {{self.line}}, column {{start_col}}")

        self.pos += 1  # Skip closing quote
        self.column += 1
        self.tokens.append(Token(TokenType.STRING, value, self.line, start_col))
        return True

    def _match_operator(self) -> bool:
        """Match operators"""
        # Try longest operators first
        for length in range(3, 0, -1):
            if self.pos + length <= len(self.source):
                op = self.source[self.pos:self.pos+length]
                if op in self.operators:
                    self.tokens.append(Token(self.operators[op], op, self.line, self.column))
                    self.pos += length
                    self.column += length
                    return True
        return False

    def _match_identifier_or_keyword(self) -> bool:
        """Match identifiers or keywords"""
        if not (self.source[self.pos].isalpha() or self.source[self.pos] == '_'):
            return False

        start = self.pos
        start_col = self.column

        while self.pos < len(self.source) and (self.source[self.pos].isalnum() or self.source[self.pos] == '_'):
            self.pos += 1
            self.column += 1

        value = self.source[start:self.pos]

        # Check if it's a keyword
        if value in self.keywords:
            token_type = self.keywords[value]
        else:
            token_type = TokenType.IDENTIFIER

        self.tokens.append(Token(token_type, value, self.line, start_col))
        return True

    def _match_symbol(self) -> bool:
        """Match single-character symbols"""
        symbols = {{
            '(': TokenType.LPAREN,
            ')': TokenType.RPAREN,
            '{{': TokenType.LBRACE,
            '}}': TokenType.RBRACE,
            '[': TokenType.LBRACKET,
            ']': TokenType.RBRACKET,
            ';': TokenType.SEMICOLON,
            ',': TokenType.COMMA,
            '.': TokenType.DOT,
        }}

        char = self.source[self.pos]
        if char in symbols:
            self.tokens.append(Token(symbols[char], char, self.line, self.column))
            self.pos += 1
            self.column += 1
            return True

        return False
'''

        # Generate keyword tokens
        keyword_tokens = "\n".join([
            f"    {kw.word.upper()} = auto()  # {kw.description}"
            for kw in self.spec.keywords
        ]) if self.spec.keywords else "    pass"

        # Generate operator tokens
        operator_tokens = "\n".join([
            f"    OP_{op.symbol.replace('+', 'PLUS').replace('-', 'MINUS').replace('*', 'STAR').replace('/', 'SLASH').replace('=', 'EQ').replace('<', 'LT').replace('>', 'GT').replace('!', 'BANG').replace('&', 'AMP').replace('|', 'PIPE').replace('%', 'MOD')} = auto()  # {op.symbol}"
            for op in self.spec.operators
        ]) if self.spec.operators else "    pass"

        # Generate keyword map
        keyword_map = ",\n".join([
            f"            '{kw.word}': TokenType.{kw.word.upper()}"
            for kw in self.spec.keywords
        ]) if self.spec.keywords else ""

        # Generate operator map
        operator_map = ",\n".join([
            f"            '{op.symbol}': TokenType.OP_{op.symbol.replace('+', 'PLUS').replace('-', 'MINUS').replace('*', 'STAR').replace('/', 'SLASH').replace('=', 'EQ').replace('<', 'LT').replace('>', 'GT').replace('!', 'BANG').replace('&', 'AMP').replace('|', 'PIPE').replace('%', 'MOD')}"
            for op in self.spec.operators
        ]) if self.spec.operators else ""

        return code.format(
            name=self.spec.name,
            keyword_tokens=keyword_tokens,
            operator_tokens=operator_tokens,
            keyword_map=keyword_map,
            operator_map=operator_map,
            single_line_comment=repr(self.spec.comment_syntax.get("single_line", "//"))
        )

    def _generate_parser(self) -> str:
        """Generate recursive descent parser"""
        code = '''"""
Parser for {name}
Auto-generated by Illiterate Wizard
"""

from typing import List, Optional
from lexer import Token, TokenType, Lexer
from ast_nodes import *


class Parser:
    def __init__(self, tokens: List[Token]):
        self.tokens = tokens
        self.pos = 0

    def parse(self) -> ProgramNode:
        """Parse the token stream into an AST"""
        statements = []
        while not self._check(TokenType.EOF):
            stmt = self._parse_statement()
            if stmt:
                statements.append(stmt)
        return ProgramNode(statements)

    def _current(self) -> Token:
        """Get current token"""
        if self.pos < len(self.tokens):
            return self.tokens[self.pos]
        return self.tokens[-1]

    def _advance(self) -> Token:
        """Move to next token"""
        token = self._current()
        if not self._check(TokenType.EOF):
            self.pos += 1
        return token

    def _check(self, token_type: TokenType) -> bool:
        """Check if current token is of given type"""
        return self._current().type == token_type

    def _match(self, *token_types: TokenType) -> bool:
        """Check and consume if current token matches any of the types"""
        for token_type in token_types:
            if self._check(token_type):
                self._advance()
                return True
        return False

    def _expect(self, token_type: TokenType, message: str) -> Token:
        """Consume token or raise error"""
        if self._check(token_type):
            return self._advance()
        raise SyntaxError(f"{{message}} at line {{self._current().line}}, got {{self._current().type}}")

    def _parse_statement(self):
        """Parse a statement"""
        # This is a template - specific statement parsing depends on grammar
        current = self._current()

        # Try to match against defined syntax rules
        {statement_parsers}

        # Default: try expression statement
        expr = self._parse_expression()
        if self._match(TokenType.SEMICOLON):
            return ExpressionStatementNode(expr)
        return expr

    def _parse_expression(self):
        """Parse an expression"""
        return self._parse_assignment()

    def _parse_assignment(self):
        """Parse assignment expression"""
        expr = self._parse_logical_or()

        if self._match(TokenType.OP_EQ):
            value = self._parse_assignment()
            return AssignmentNode(expr, value)

        return expr

    def _parse_logical_or(self):
        """Parse logical OR expression"""
        expr = self._parse_logical_and()

        while self._match(TokenType.OP_PIPE, TokenType.OP_PIPE):
            op = self.tokens[self.pos - 1].value
            right = self._parse_logical_and()
            expr = BinaryOpNode(expr, op, right)

        return expr

    def _parse_logical_and(self):
        """Parse logical AND expression"""
        expr = self._parse_equality()

        while self._match(TokenType.OP_AMP, TokenType.OP_AMP):
            op = self.tokens[self.pos - 1].value
            right = self._parse_equality()
            expr = BinaryOpNode(expr, op, right)

        return expr

    def _parse_equality(self):
        """Parse equality expression"""
        expr = self._parse_comparison()

        while self._match(TokenType.OP_EQEQ, TokenType.OP_BANGEQ):
            op = self.tokens[self.pos - 1].value
            right = self._parse_comparison()
            expr = BinaryOpNode(expr, op, right)

        return expr

    def _parse_comparison(self):
        """Parse comparison expression"""
        expr = self._parse_addition()

        while self._match(TokenType.OP_LT, TokenType.OP_GT, TokenType.OP_LTEQ, TokenType.OP_GTEQ):
            op = self.tokens[self.pos - 1].value
            right = self._parse_addition()
            expr = BinaryOpNode(expr, op, right)

        return expr

    def _parse_addition(self):
        """Parse addition/subtraction"""
        expr = self._parse_multiplication()

        while self._match(TokenType.OP_PLUS, TokenType.OP_MINUS):
            op = self.tokens[self.pos - 1].value
            right = self._parse_multiplication()
            expr = BinaryOpNode(expr, op, right)

        return expr

    def _parse_multiplication(self):
        """Parse multiplication/division"""
        expr = self._parse_unary()

        while self._match(TokenType.OP_STAR, TokenType.OP_SLASH, TokenType.OP_MOD):
            op = self.tokens[self.pos - 1].value
            right = self._parse_unary()
            expr = BinaryOpNode(expr, op, right)

        return expr

    def _parse_unary(self):
        """Parse unary expression"""
        if self._match(TokenType.OP_BANG, TokenType.OP_MINUS):
            op = self.tokens[self.pos - 1].value
            expr = self._parse_unary()
            return UnaryOpNode(op, expr)

        return self._parse_primary()

    def _parse_primary(self):
        """Parse primary expression"""
        # Literals
        if self._check(TokenType.INTEGER):
            value = self._advance().value
            return LiteralNode(int(value))

        if self._check(TokenType.FLOAT):
            value = self._advance().value
            return LiteralNode(float(value))

        if self._check(TokenType.STRING):
            value = self._advance().value
            return LiteralNode(value)

        # Identifiers
        if self._check(TokenType.IDENTIFIER):
            name = self._advance().value

            # Function call
            if self._match(TokenType.LPAREN):
                args = []
                if not self._check(TokenType.RPAREN):
                    args.append(self._parse_expression())
                    while self._match(TokenType.COMMA):
                        args.append(self._parse_expression())
                self._expect(TokenType.RPAREN, "Expected ')' after arguments")
                return FunctionCallNode(name, args)

            return IdentifierNode(name)

        # Grouping
        if self._match(TokenType.LPAREN):
            expr = self._parse_expression()
            self._expect(TokenType.RPAREN, "Expected ')' after expression")
            return expr

        raise SyntaxError(f"Unexpected token {{self._current().value}} at line {{self._current().line}}")
'''

        # Generate statement parsers based on syntax rules
        statement_parsers = []
        for rule in self.spec.syntax_rules:
            if rule.rule_type in ["statement", "declaration"]:
                # Generate parser stub for this rule
                statement_parsers.append(f"        # TODO: Parse {rule.rule_type}: {rule.pattern}")

        return code.format(
            name=self.spec.name,
            statement_parsers="\n".join(statement_parsers) if statement_parsers else "        pass"
        )

    def _generate_ast_nodes(self) -> str:
        """Generate AST node class definitions"""
        code = '''"""
AST Node definitions for {name}
Auto-generated by Illiterate Wizard
"""

from dataclasses import dataclass
from typing import Any, List, Optional


@dataclass
class ASTNode:
    """Base class for all AST nodes"""
    pass


@dataclass
class ProgramNode(ASTNode):
    """Root node of the AST"""
    statements: List[ASTNode]


@dataclass
class LiteralNode(ASTNode):
    """Literal value node"""
    value: Any


@dataclass
class IdentifierNode(ASTNode):
    """Identifier/variable reference"""
    name: str


@dataclass
class BinaryOpNode(ASTNode):
    """Binary operation"""
    left: ASTNode
    operator: str
    right: ASTNode


@dataclass
class UnaryOpNode(ASTNode):
    """Unary operation"""
    operator: str
    operand: ASTNode


@dataclass
class AssignmentNode(ASTNode):
    """Assignment expression"""
    target: ASTNode
    value: ASTNode


@dataclass
class FunctionCallNode(ASTNode):
    """Function call"""
    name: str
    arguments: List[ASTNode]


@dataclass
class ExpressionStatementNode(ASTNode):
    """Expression as a statement"""
    expression: ASTNode


@dataclass
class BlockNode(ASTNode):
    """Block of statements"""
    statements: List[ASTNode]


@dataclass
class IfNode(ASTNode):
    """If statement"""
    condition: ASTNode
    then_branch: ASTNode
    else_branch: Optional[ASTNode] = None


@dataclass
class WhileNode(ASTNode):
    """While loop"""
    condition: ASTNode
    body: ASTNode


@dataclass
class ForNode(ASTNode):
    """For loop"""
    initializer: Optional[ASTNode]
    condition: Optional[ASTNode]
    increment: Optional[ASTNode]
    body: ASTNode


@dataclass
class FunctionDefNode(ASTNode):
    """Function definition"""
    name: str
    parameters: List[str]
    body: ASTNode


@dataclass
class ReturnNode(ASTNode):
    """Return statement"""
    value: Optional[ASTNode] = None


@dataclass
class VariableDeclarationNode(ASTNode):
    """Variable declaration"""
    name: str
    type_annotation: Optional[str] = None
    initializer: Optional[ASTNode] = None
'''

        return code.format(name=self.spec.name)
